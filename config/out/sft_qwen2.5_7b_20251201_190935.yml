model: Qwen/Qwen2.5-7B-Instruct
model_type: null
model_revision: null
task_type: causal_lm
torch_dtype: bfloat16
attn_impl: flash_attention_2
new_special_tokens: []
num_labels: null
problem_type: null
rope_scaling: null
max_model_len: null
device_map: auto
max_memory: null
local_repo_path: null
init_strategy: null
model_kwargs: {}
dataset:
- your_dataset_id_or_path:default#1
val_dataset: []
cached_dataset: []
split_dataset_ratio: 0.1
data_seed: 42
dataset_num_proc: 1
load_from_cache_file: false
dataset_shuffle: true
val_dataset_shuffle: false
streaming: false
interleave_prob: null
stopping_strategy: first_exhausted
shuffle_buffer_size: 1000
download_mode: reuse_dataset_if_exists
columns:
  messages: messages
strict: false
remove_unused_columns: true
auto_fields:
  model_name: null
  model_author: null
dataset_info_files: []
template: chatml
system: You are a helpful assistant.
max_length: 4096
truncation_strategy: longest_first
padding_free: false
packing: false
s2s: false
max_pixels: null
image_aspect_ratio: pad
loss_scale: null
label_smoothing: 0.0
ignore_pad_token_for_loss: true
prompt_field: prompt
response_field: response
system_field: system
messages_field: messages
trainer: seq2seq
stage: sft
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
max_steps: 2000
num_train_epochs: 1.0
warmup_ratio: 0.03
warmup_steps: 0
save_steps: 200
logging_steps: 10
eval_steps: 200
learning_rate: 2.0e-05
weight_decay: 0.0
lr_scheduler_type: cosine
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
gradient_checkpointing: false
logging_strategy: steps
evaluation_strategy: steps
save_strategy: steps
save_total_limit: 3
report_to:
- tensorboard
load_best_model_at_end: false
metric_for_best_model: loss
greater_is_better: false
fp16: false
bf16: false
tf32: null
deepspeed: null
fsdp: null
fsdp_config: {}
group_by_length: false
length_column_name: null
resume_from_checkpoint: null
max_new_tokens: 512
temperature: 0.7
top_p: 0.9
top_k: 50
repetition_penalty: 1.0
presence_penalty: 0.0
frequency_penalty: 0.0
early_stopping: false
length_penalty: 1.0
no_repeat_ngram_size: 0
beam_search: false
num_beams: 1
diversity_penalty: 0.0
beam_group: 1
do_sample: true
use_cache: true
stream: false
stop_words: []
logprobs: null
n_tokens: null
quant_method: none
bnb_4bit: false
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: true
bnb_8bit: false
bnb_quant_type: null
q_bits: 4
q_group_size: 128
q_desc_act: false
q_dtype: int4
q_apply_lora: true
kv_quant: false
kv_bits: 4
kv_group_size: 256
kv_desc_act: false
