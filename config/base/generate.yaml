# 基础：推理/生成参数（Decoding 与采样）

# 生成长度与采样分布
max_new_tokens: 512           # 最大新生成 token 数。
temperature: 0.7              # 温度采样（>1 更发散，<1 更保守）。 保留概率最高的top_k数量 tokens用于生成，默认为None。读取'generation_config.json'。
top_p: 0.9                    # nucleus sampling：累积概率阈值。 保留概率最高的累计概率达到 top_p 的tokens用于生成，默认为None。读取generation_config.json。
top_k: 50                     # 只在 top-k 候选中采样。
repetition_penalty: 1.0       # 重复惩罚（>1 惩罚重复）。1.0 表示不进行惩罚。默认为None，读取generation_config.json。

# 搜索/beam 相关
num_beams: 1                  # beam search的并行保留数量，默认为1。

# 其他生成行为
stream: false                 # 流式输出（需要前端/引擎支持）。，默认为None，即使用交互式界面时为True，数据集批量推理时为False。"ms-swift<3.6"stream默认值为False。
stop_words: []                # 除了eos_token外额外的停止词，默认为[]。注意：eos_token会在输出respsone中被删除，额外停止词会在输出中保留。

# 引擎专属（如 vLLM/SGLang）
logprobs: null                # 是否返回 token 级对数概率（布尔或与引擎语义一致）。
top_logprobs: null            # 返回每步 top-k 的概率明细（优先使用该键）。
